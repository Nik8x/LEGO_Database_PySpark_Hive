{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LEGO Database.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nik8x/LEGO_Database_PySpark_Hive/blob/master/LEGO_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZqqH7V5Cj7Z",
        "colab_type": "text"
      },
      "source": [
        "# LEGO Database\n",
        "**The LEGO Parts/Sets/Colors and Inventories of every official LEGO set**\n",
        "\n",
        "LEGO is a popular brand of toy building bricks. They are often sold in sets with in order to build a specific object. Each set contains a number of parts in different shapes, sizes and colors. This database contains information on which parts are included in different LEGO sets. It was originally compiled to help people who owned some LEGO sets already figure out what other sets they could build with the pieces they had.\n",
        "\n",
        "\n",
        "\n",
        "*   How have the size of sets changed over time?\n",
        "*   What colors are associated with witch themes? Could you predict which theme a set is from just by the bricks it contains?\n",
        "*   What sets have the most-used pieces in them? What sets have the rarest pieces in them?\n",
        "*   Have the colors of LEGOs included in sets changed over time?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMawFk9HBCv5",
        "colab_type": "text"
      },
      "source": [
        "![Schema Diagram for LEGO datafiles](https://rebrickable.com/static/img/diagrams/downloads_schema_v2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxbH7YVIBOf3",
        "colab_type": "text"
      },
      "source": [
        "[Kaggle Link](https://www.kaggle.com/rtatman/lego-database#colors.csv)\n",
        "\n",
        "[LEGO Database Download](https://rebrickable.com/downloads/)\n",
        "\n",
        "[themes.csv ](https://rebrickable.com/media/downloads/themes.csv?1557727192.3575437)\n",
        "\n",
        "[colors.csv ](https://rebrickable.com/media/downloads/colors.csv?1557727194.8508914)\n",
        "\n",
        "[part_categories.csv ](https://rebrickable.com/media/downloads/part_categories.csv?1557727188.1475196)\n",
        "\n",
        "[parts.csv](https://rebrickable.com/media/downloads/parts.csv?1557727193.5342171) \n",
        "\n",
        "[inventories.csv ](https://rebrickable.com/media/downloads/inventories.csv?1557727192.2342098)\n",
        "\n",
        "[sets.csv ](https://rebrickable.com/media/downloads/sets.csv?1557727193.8908858)\n",
        "\n",
        "[inventory_parts.csv](https://rebrickable.com/media/downloads/sets.csv?1557727193.8908858)\n",
        "\n",
        "[inventory_sets.csv ](https://rebrickable.com/media/downloads/inventory_sets.csv?1557727194.9708922)\n",
        "\n",
        "[part_relationships.csv ](https://rebrickable.com/media/downloads/part_relationships.csv?1557727194.0842204)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJa6f4UXE4dt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "09b47b59-f05a-4a47-b568-ce6190fafc47"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/98/244399c0daa7894cdf387e7007d5e8b3710a79b67f3fd991c0b0b644822d/pyspark-2.4.3.tar.gz (215.6MB)\n",
            "\u001b[K     |████████████████████████████████| 215.6MB 93kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 39.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/20/f0/b30e2024226dc112e256930dd2cd4f06d00ab053c86278dcf3\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skXsoJMiF4ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.0-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx6n9hUOFTTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark # only run after findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "from pyspark.sql import Row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu9jEULfq8Kd",
        "colab_type": "text"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSiWoQb2BJ2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://rebrickable.com/media/downloads/themes.csv?1557727192.3575437\n",
        "!wget https://rebrickable.com/media/downloads/colors.csv?1557727194.8508914\n",
        "!wget https://rebrickable.com/media/downloads/part_categories.csv?1557727188.1475196\n",
        "!wget https://rebrickable.com/media/downloads/parts.csv?1557727193.5342171\n",
        "!wget https://rebrickable.com/media/downloads/inventories.csv?1557727192.2342098\n",
        "!wget https://rebrickable.com/media/downloads/sets.csv?1557727193.8908858\n",
        "!wget https://rebrickable.com/media/downloads/sets.csv?1557727193.8908858\n",
        "!wget https://rebrickable.com/media/downloads/inventory_sets.csv?1557727194.9708922\n",
        "!wget https://rebrickable.com/media/downloads/part_relationships.csv?1557727194.0842204  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skoyKhi0Do0S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "87a96867-1a48-4ce4-cefe-0e1cef5a980f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'colors.csv?1557727194.8508914'\t\t     'parts.csv?1557727193.5342171'\n",
            "'inventories.csv?1557727192.2342098'\t      sample_data\n",
            "'inventory_sets.csv?1557727194.9708922'      'sets.csv?1557727193.8908858'\n",
            "'part_categories.csv?1557727188.1475196'     'sets.csv?1557727193.8908858.1'\n",
            "'part_relationships.csv?1557727194.0842204'  'themes.csv?1557727192.3575437'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yiDw0a-F1Jh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "897ee781-5025-42d5-f71d-f826d5562fe1"
      },
      "source": [
        "os.listdir(os.getcwd())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'parts.csv?1557727193.5342171',\n",
              " 'part_relationships.csv?1557727194.0842204',\n",
              " 'inventories.csv?1557727192.2342098',\n",
              " 'spark-2.3.0-bin-hadoop2.7',\n",
              " 'inventory_sets.csv?1557727194.9708922',\n",
              " '.ipynb_checkpoints',\n",
              " 'sets.csv?1557727193.8908858',\n",
              " 'sets.csv?1557727193.8908858.1',\n",
              " 'themes.csv?1557727192.3575437',\n",
              " 'colors.csv?1557727194.8508914',\n",
              " 'part_categories.csv?1557727188.1475196',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VYIOk8wrAFp",
        "colab_type": "text"
      },
      "source": [
        "### Create Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0chTGv4I1v-",
        "colab_type": "text"
      },
      "source": [
        "we can use Hive commands to see databases and tables. \n",
        "\n",
        "However, at this point, we do not have any database or table. We will create them below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vu4T7QyH9wW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4a7b8bb9-0d8b-4684-87de-1ef0944472d9"
      },
      "source": [
        "spark.sql('show databases').show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|databaseName|\n",
            "+------------+\n",
            "|     default|\n",
            "+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahCXyOYSISbs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "33440caf-4cbb-488a-f5ef-6dd4f74cadd3"
      },
      "source": [
        "spark.sql('show tables').show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------+-----------+\n",
            "|database|tableName|isTemporary|\n",
            "+--------+---------+-----------+\n",
            "+--------+---------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVYyBhdmIavf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2510502-a8e6-428d-e029-be695ab0a32c"
      },
      "source": [
        "functions =  spark.sql('show functions').collect() # total functions in Spark.SQL\n",
        "len(functions)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "267"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nzBUNMFIutM",
        "colab_type": "text"
      },
      "source": [
        "Now, let's create a database. The data we will use is LEGO Database. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5nC8uMaImqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cb91836-546d-4a45-82de-4978b41db9e4"
      },
      "source": [
        "spark.sql('create database lego')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY5iti-GI8vT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "477c34d9-41bf-41ac-c5a3-3ebb1c7ee3ec"
      },
      "source": [
        "# Let's check if our database has been created.\n",
        "\n",
        "spark.sql('show databases').show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|databaseName|\n",
            "+------------+\n",
            "|     default|\n",
            "|        lego|\n",
            "+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ATsHPocqrkP",
        "colab_type": "text"
      },
      "source": [
        "### Create Tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGoi3v3_JIA2",
        "colab_type": "text"
      },
      "source": [
        "Now, let's create tables: in textfile format, in ORC and in AVRO format. \n",
        "But first, we have to make sure we are using the movies database by switching to it using the command below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS3cM0JBI_PP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14bb3212-240c-4d0e-d343-200caea18bb1"
      },
      "source": [
        "spark.sql('use lego')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzfSPCc5JY-c",
        "colab_type": "text"
      },
      "source": [
        "The lego dataset has many fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V0qDl3KJYUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b9c389e-54c3-429b-d931-ff5ada0bf687"
      },
      "source": [
        "spark.sql('create table colors \\\n",
        "         (id int,name string,rgb string, is_trans string) \\\n",
        "         row format delimited fields terminated by \",\"\\\n",
        "         stored as textfile')  \n",
        "\n",
        "spark.sql('create table inventories \\\n",
        "         (idUnique int,version int,set_num int) \\\n",
        "         row format delimited fields terminated by \",\"\\\n",
        "         stored as textfile')  \n",
        "\n",
        "spark.sql(\"create table inventory_parts\\\n",
        "           (inventory_id int,part_num string,color_id int,quantity int, is_spare string)\\\n",
        "           stored as ORC\")\n",
        "\n",
        "spark.sql(\"create table inventory_sets\\\n",
        "           (inventory_id int,set_num string,quantity int)\\\n",
        "           stored as ORC\" ) \n",
        "\n",
        "spark.sql(\"create table part_categories\\\n",
        "           (id int,name string)\\\n",
        "           stored as AVRO\") \n",
        "\n",
        "spark.sql(\"create table parts\\\n",
        "           ( part_num string,name string, part_cat_id int)\\\n",
        "           stored as AVRO\") \n",
        "\n",
        "spark.sql(\"create table sets\\\n",
        "           ( set_num string,name string, year int, theme_id int, num_parts int)\\\n",
        "           stored as sequencefile\") \n",
        "\n",
        "spark.sql(\"create table themes\\\n",
        "           (id int, name string, parent_id int)\\\n",
        "           stored as parquet\") "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGVf3OLVJSWC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "ed3b418c-8252-40d0-8500-cb7d545472f5"
      },
      "source": [
        "spark.sql('show tables').show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------------+-----------+\n",
            "|database|      tableName|isTemporary|\n",
            "+--------+---------------+-----------+\n",
            "|    lego|         colors|      false|\n",
            "|    lego|    inventories|      false|\n",
            "|    lego|inventory_parts|      false|\n",
            "|    lego| inventory_sets|      false|\n",
            "|    lego|part_categories|      false|\n",
            "|    lego|          parts|      false|\n",
            "|    lego|           sets|      false|\n",
            "|    lego|         themes|      false|\n",
            "+--------+---------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKxcLdtlfglf",
        "colab_type": "text"
      },
      "source": [
        "All the tables we created above.\n",
        "\n",
        "We can get information about a table as below. If we do not include formatted or extended in the command, we see only information about the columns. But now, we see even its location, the database and other attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx5-2pTTfE6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "8d51622c-3e82-4e0b-904f-36110a4c7d5c"
      },
      "source": [
        "spark.sql(\"describe formatted inventories\").show(truncate = False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------+----------------------------------------------------------+-------+\n",
            "|col_name                    |data_type                                                 |comment|\n",
            "+----------------------------+----------------------------------------------------------+-------+\n",
            "|idUnique                    |int                                                       |null   |\n",
            "|version                     |int                                                       |null   |\n",
            "|set_num                     |int                                                       |null   |\n",
            "|                            |                                                          |       |\n",
            "|# Detailed Table Information|                                                          |       |\n",
            "|Database                    |lego                                                      |       |\n",
            "|Table                       |inventories                                               |       |\n",
            "|Owner                       |root                                                      |       |\n",
            "|Created Time                |Wed May 15 03:04:29 UTC 2019                              |       |\n",
            "|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                              |       |\n",
            "|Created By                  |Spark 2.3.0                                               |       |\n",
            "|Type                        |MANAGED                                                   |       |\n",
            "|Provider                    |hive                                                      |       |\n",
            "|Table Properties            |[transient_lastDdlTime=1557889469]                        |       |\n",
            "|Location                    |file:/content/spark-warehouse/lego.db/inventories         |       |\n",
            "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe        |       |\n",
            "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                  |       |\n",
            "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat|       |\n",
            "|Storage Properties          |[field.delim=,, serialization.format=,]                   |       |\n",
            "|Partition Provider          |Catalog                                                   |       |\n",
            "+----------------------------+----------------------------------------------------------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJJ3m5mVqjkI",
        "colab_type": "text"
      },
      "source": [
        "### Load Data using Hive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v1OpxANghz6",
        "colab_type": "text"
      },
      "source": [
        "Now let's load data to the movies table. We can load data from a local file system or from any hadoop supported file system. If we are loading it just one time, we do not need to include overwrite. However, if there is possiblity that we could run the code more than one time, including overwrite is important not to append the same dataset to the table again and again. Hive does not do any transformation while loading data into tables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-lBanC2jcVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv colors.csv\\?1557727194.8508914 colors.csv\n",
        "!mv inventories.csv\\?1557727192.2342098 inventories.csv\n",
        "!mv inventory_sets.csv\\?1557727194.9708922 inventory_sets.csv\n",
        "!mv part_categories.csv\\?1557727188.1475196 part_categories.csv\n",
        "!mv parts.csv\\?1557727193.5342171 parts.csv\n",
        "!mv sets.csv\\?1557727193.8908858 sets.csv\n",
        "!mv themes.csv\\?1557727192.3575437 themes.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvQMJBx_fr_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4082df04-dd7d-4135-cbff-ca6739a301fa"
      },
      "source": [
        "spark.sql(\"load data local inpath '/content/colors.csv' \\\n",
        "           overwrite into table colors\")\n",
        "\n",
        "spark.sql(\"load data local inpath '/content/inventories.csv' \\\n",
        "            overwrite into table inventories\")\n",
        "\n",
        "spark.sql(\"load data local inpath '/content/inventory_sets.csv' \\\n",
        "            overwrite into table inventory_sets\")\n",
        "\n",
        "spark.sql(\"load data local inpath '/content/part_categories.csv' \\\n",
        "            overwrite into table part_categories\")\n",
        "\n",
        "spark.sql(\"load data local inpath '/content/parts.csv' \\\n",
        "            overwrite into table parts\")\n",
        "\n",
        "spark.sql(\"load data local inpath '/content/sets.csv' \\\n",
        "            overwrite into table sets\")\n",
        "\n",
        "spark.sql(\"load data local inpath '/content/themes_1.csv' \\\n",
        "            overwrite into table themes\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1KadSwPh9TK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "c952bcb3-d9d6-443c-ef12-e37a95284cf4"
      },
      "source": [
        "spark.sql(\"show create table inventory_sets\").show(truncate = False)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|CREATE TABLE `inventory_sets`(`inventory_id` int, `set_num` string, `quantity` int)\n",
            "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\n",
            "WITH SERDEPROPERTIES (\n",
            "  'serialization.format' = '1'\n",
            ")\n",
            "STORED AS\n",
            "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\n",
            "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\n",
            "TBLPROPERTIES (\n",
            "  'transient_lastDdlTime' = '1557892237'\n",
            ")\n",
            "|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQZU0SEhk_qV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "e5c8e6b4-b7b0-4c16-fc37-2e316238653b"
      },
      "source": [
        "spark.sql(\"select * from colors limit 5\").show(truncate = False)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---------+------+--------+\n",
            "|id  |name     |rgb   |is_trans|\n",
            "+----+---------+------+--------+\n",
            "|null|name     |rgb   |is_trans|\n",
            "|-1  |[Unknown]|0033B2|f       |\n",
            "|0   |Black    |05131D|f       |\n",
            "|1   |Blue     |0055BF|f       |\n",
            "|2   |Green    |237841|f       |\n",
            "+----+---------+------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LkDk2lnqWJn",
        "colab_type": "text"
      },
      "source": [
        "### Load Data as Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILtQ1W7TroJF",
        "colab_type": "text"
      },
      "source": [
        "Rather than loading the data as a bulk, we can pre-process it and create a dataframe and insert our dataframe to the table.\n",
        "\n",
        "We can create dataframes in two ways.\n",
        "\n",
        "* by using the Spark SQL read function such as spark.read.csv, spark.read.json, spark.read.orc, spark.read.avro, spark.read.parquet, etc.\n",
        "* by reading it in as an RDD and converting it to a dataframe after pre-processing it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFLgH7Erry88",
        "colab_type": "text"
      },
      "source": [
        "Let's specify schema for the inventory_parts dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueRiLRJCqQ1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}